{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "489c476d",
   "metadata": {
    "papermill": {
     "duration": 0.010232,
     "end_time": "2023-07-23T08:32:38.817719",
     "exception": false,
     "start_time": "2023-07-23T08:32:38.807487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Contents\n",
    "\n",
    "In this Notebook I will start with the very Basics of RNN's and Build all the way to latest deep learning architectures to solve NLP problems. It will cover the Following:\n",
    "* Simple RNN's\n",
    "* Word Embeddings : Definition and How to get them\n",
    "* LSTM's\n",
    "* GRU's\n",
    "\n",
    "I will divide every Topic into four subsections:\n",
    "* Basic Overview\n",
    "* In-Depth Understanding : In this I will attach links of articles and videos to learn about the topic in depth\n",
    "* Code-Implementation\n",
    "* Code Explanation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17bea750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:32:38.835680Z",
     "iopub.status.busy": "2023-07-23T08:32:38.834507Z",
     "iopub.status.idle": "2023-07-23T08:33:10.293627Z",
     "shell.execute_reply": "2023-07-23T08:33:10.292154Z"
    },
    "papermill": {
     "duration": 31.471008,
     "end_time": "2023-07-23T08:33:10.296611",
     "exception": false,
     "start_time": "2023-07-23T08:32:38.825603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (0.12.2)\r\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from seaborn) (1.23.5)\r\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.10/site-packages (from seaborn) (1.5.3)\r\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.10/site-packages (from seaborn) (3.7.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.40.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9d014b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:33:10.315201Z",
     "iopub.status.busy": "2023-07-23T08:33:10.314591Z",
     "iopub.status.idle": "2023-07-23T08:33:40.003636Z",
     "shell.execute_reply": "2023-07-23T08:33:40.001875Z"
    },
    "papermill": {
     "duration": 29.70138,
     "end_time": "2023-07-23T08:33:40.006192",
     "exception": false,
     "start_time": "2023-07-23T08:33:10.304812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.15.0)\r\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (8.2.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from plotly) (21.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->plotly) (3.0.9)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb5f9d6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-07-23T08:33:40.025005Z",
     "iopub.status.busy": "2023-07-23T08:33:40.024633Z",
     "iopub.status.idle": "2023-07-23T08:33:52.444230Z",
     "shell.execute_reply": "2023-07-23T08:33:52.442973Z"
    },
    "papermill": {
     "duration": 12.432426,
     "end_time": "2023-07-23T08:33:52.447043",
     "exception": false,
     "start_time": "2023-07-23T08:33:40.014617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding,BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8477a4d",
   "metadata": {
    "papermill": {
     "duration": 0.008111,
     "end_time": "2023-07-23T08:33:52.463897",
     "exception": false,
     "start_time": "2023-07-23T08:33:52.455786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configuring TPU's\n",
    "\n",
    "For this version of Notebook we will be using TPU's as we have to built a BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1756233a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:33:52.482607Z",
     "iopub.status.busy": "2023-07-23T08:33:52.482195Z",
     "iopub.status.idle": "2023-07-23T08:33:52.503121Z",
     "shell.execute_reply": "2023-07-23T08:33:52.501801Z"
    },
    "papermill": {
     "duration": 0.033186,
     "end_time": "2023-07-23T08:33:52.505597",
     "exception": false,
     "start_time": "2023-07-23T08:33:52.472411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "    # set: this is always the case on Kaggle.\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d35e0bad",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2023-07-23T08:33:52.524186Z",
     "iopub.status.busy": "2023-07-23T08:33:52.523789Z",
     "iopub.status.idle": "2023-07-23T08:33:56.047360Z",
     "shell.execute_reply": "2023-07-23T08:33:56.046020Z"
    },
    "papermill": {
     "duration": 3.535747,
     "end_time": "2023-07-23T08:33:56.049902",
     "exception": false,
     "start_time": "2023-07-23T08:33:52.514155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv')\n",
    "validation = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
    "test = pd.read_csv('/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307ee4c",
   "metadata": {
    "papermill": {
     "duration": 0.008564,
     "end_time": "2023-07-23T08:33:56.067088",
     "exception": false,
     "start_time": "2023-07-23T08:33:56.058524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will drop the other columns and approach this problem as a Binary Classification Problem and also we will have our exercise done on a smaller subsection of the dataset(only 12000 data points) to make it easier to train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bde6ba6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:33:56.087550Z",
     "iopub.status.busy": "2023-07-23T08:33:56.087147Z",
     "iopub.status.idle": "2023-07-23T08:33:56.118901Z",
     "shell.execute_reply": "2023-07-23T08:33:56.117753Z"
    },
    "papermill": {
     "duration": 0.045194,
     "end_time": "2023-07-23T08:33:56.121497",
     "exception": false,
     "start_time": "2023-07-23T08:33:56.076303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681bf66a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:33:56.142103Z",
     "iopub.status.busy": "2023-07-23T08:33:56.141745Z",
     "iopub.status.idle": "2023-07-23T08:33:56.151008Z",
     "shell.execute_reply": "2023-07-23T08:33:56.149273Z"
    },
    "papermill": {
     "duration": 0.023467,
     "end_time": "2023-07-23T08:33:56.154016",
     "exception": false,
     "start_time": "2023-07-23T08:33:56.130549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12001, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.loc[:12000,:]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7a1c6e",
   "metadata": {
    "papermill": {
     "duration": 0.008822,
     "end_time": "2023-07-23T08:33:56.172560",
     "exception": false,
     "start_time": "2023-07-23T08:33:56.163738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will check the maximum number of words that can be present in a comment , this will help us in padding later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2957c77f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:33:56.192995Z",
     "iopub.status.busy": "2023-07-23T08:33:56.192398Z",
     "iopub.status.idle": "2023-07-23T08:33:56.246811Z",
     "shell.execute_reply": "2023-07-23T08:33:56.245553Z"
    },
    "papermill": {
     "duration": 0.067592,
     "end_time": "2023-07-23T08:33:56.249230",
     "exception": false,
     "start_time": "2023-07-23T08:33:56.181638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1403"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text'].apply(lambda x:len(str(x).split())).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9538e2",
   "metadata": {
    "papermill": {
     "duration": 0.008304,
     "end_time": "2023-07-23T08:33:56.266579",
     "exception": false,
     "start_time": "2023-07-23T08:33:56.258275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Writing a function for getting auc score for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b200849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:33:56.286266Z",
     "iopub.status.busy": "2023-07-23T08:33:56.285875Z",
     "iopub.status.idle": "2023-07-23T08:33:56.291749Z",
     "shell.execute_reply": "2023-07-23T08:33:56.290748Z"
    },
    "papermill": {
     "duration": 0.018545,
     "end_time": "2023-07-23T08:33:56.293527",
     "exception": false,
     "start_time": "2023-07-23T08:33:56.274982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def roc_auc(predictions,target):\n",
    "    '''\n",
    "    This methods returns the AUC Score when given the Predictions\n",
    "    and Labels\n",
    "    '''\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b679d19",
   "metadata": {
    "papermill": {
     "duration": 0.008376,
     "end_time": "2023-07-23T08:33:56.310557",
     "exception": false,
     "start_time": "2023-07-23T08:33:56.302181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6988b32d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:33:56.329673Z",
     "iopub.status.busy": "2023-07-23T08:33:56.329258Z",
     "iopub.status.idle": "2023-07-23T08:33:56.344020Z",
     "shell.execute_reply": "2023-07-23T08:33:56.342462Z"
    },
    "papermill": {
     "duration": 0.027733,
     "end_time": "2023-07-23T08:33:56.346836",
     "exception": false,
     "start_time": "2023-07-23T08:33:56.319103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train.comment_text.values, train.toxic.values, \n",
    "                                                  stratify=train.toxic.values, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e09743",
   "metadata": {
    "papermill": {
     "duration": 0.008679,
     "end_time": "2023-07-23T08:33:56.364405",
     "exception": false,
     "start_time": "2023-07-23T08:33:56.355726",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Simple RNN\n",
    "\n",
    "## Basic Overview\n",
    "\n",
    "What is a RNN?\n",
    "\n",
    "Recurrent Neural Network(RNN) are a type of Neural Network where the output from previous step are fed as input to the current step. In traditional neural networks, all the inputs and outputs are independent of each other, but in cases like when it is required to predict the next word of a sentence, the previous words are required and hence there is a need to remember the previous words. Thus RNN came into existence, which solved this issue with the help of a Hidden Layer.\n",
    "\n",
    "Why RNN's?\n",
    "\n",
    "https://www.quora.com/Why-do-we-use-an-RNN-instead-of-a-simple-neural-network\n",
    "\n",
    "## In-Depth Understanding\n",
    "\n",
    "* https://medium.com/mindorks/understanding-the-recurrent-neural-network-44d593f112a2\n",
    "* https://www.youtube.com/watch?v=2E65LDnM2cA&list=PL1F3ABbhcqa3BBWo170U4Ev2wfsF7FN8l\n",
    "* https://www.d2l.ai/chapter_recurrent-neural-networks/rnn.html\n",
    "\n",
    "## Code Implementation\n",
    "\n",
    "So first I will implement the and then I will explain the code step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a02788c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:33:56.385656Z",
     "iopub.status.busy": "2023-07-23T08:33:56.385282Z",
     "iopub.status.idle": "2023-07-23T08:33:57.927633Z",
     "shell.execute_reply": "2023-07-23T08:33:57.926602Z"
    },
    "papermill": {
     "duration": 1.556759,
     "end_time": "2023-07-23T08:33:57.930273",
     "exception": false,
     "start_time": "2023-07-23T08:33:56.373514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# using keras tokenizer here\n",
    "token = text.Tokenizer(num_words=None)\n",
    "max_len = 1500\n",
    "token.fit_on_texts(list(xtrain) + list(xvalid))\n",
    "xtrain_seq = token.texts_to_sequences(xtrain)\n",
    "xvalid_seq = token.texts_to_sequences(xvalid)\n",
    "#zero pad the sequences\n",
    "xtrain_pad = pad_sequences(xtrain_seq, maxlen=max_len)\n",
    "xvalid_pad = pad_sequences(xvalid_seq, maxlen=max_len)\n",
    "word_index = token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ba6d2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:33:57.950639Z",
     "iopub.status.busy": "2023-07-23T08:33:57.950273Z",
     "iopub.status.idle": "2023-07-23T08:33:58.339395Z",
     "shell.execute_reply": "2023-07-23T08:33:58.338118Z"
    },
    "papermill": {
     "duration": 0.402174,
     "end_time": "2023-07-23T08:33:58.341746",
     "exception": false,
     "start_time": "2023-07-23T08:33:57.939572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1500, 300)         13049100  \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 100)               40100     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,089,301\n",
      "Trainable params: 13,089,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 343 ms, sys: 103 ms, total: 445 ms\n",
      "Wall time: 382 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # A simpleRNN without any pretrained embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     input_length=max_len))\n",
    "    model.add(SimpleRNN(100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9f1950c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:33:58.362090Z",
     "iopub.status.busy": "2023-07-23T08:33:58.361729Z",
     "iopub.status.idle": "2023-07-23T08:46:39.438245Z",
     "shell.execute_reply": "2023-07-23T08:46:39.437423Z"
    },
    "papermill": {
     "duration": 761.089714,
     "end_time": "2023-07-23T08:46:39.440798",
     "exception": false,
     "start_time": "2023-07-23T08:33:58.351084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "150/150 [==============================] - 150s 989ms/step - loss: 0.3052 - accuracy: 0.9014\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 159s 1s/step - loss: 0.1273 - accuracy: 0.9544\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 150s 1000ms/step - loss: 0.0172 - accuracy: 0.9954\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 151s 1s/step - loss: 0.0028 - accuracy: 0.9997\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 150s 1s/step - loss: 8.6303e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7e6aa8487940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64*strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e40e542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:46:39.551526Z",
     "iopub.status.busy": "2023-07-23T08:46:39.551130Z",
     "iopub.status.idle": "2023-07-23T08:46:50.004695Z",
     "shell.execute_reply": "2023-07-23T08:46:50.003193Z"
    },
    "papermill": {
     "duration": 10.512581,
     "end_time": "2023-07-23T08:46:50.007114",
     "exception": false,
     "start_time": "2023-07-23T08:46:39.494533",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 9s 122ms/step\n",
      "Auc: 0.87%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51fec439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:46:50.128309Z",
     "iopub.status.busy": "2023-07-23T08:46:50.127888Z",
     "iopub.status.idle": "2023-07-23T08:46:50.134325Z",
     "shell.execute_reply": "2023-07-23T08:46:50.133154Z"
    },
    "papermill": {
     "duration": 0.070911,
     "end_time": "2023-07-23T08:46:50.137509",
     "exception": false,
     "start_time": "2023-07-23T08:46:50.066598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_model = []\n",
    "scores_model.append({'Model': 'SimpleRNN','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4da554",
   "metadata": {
    "papermill": {
     "duration": 0.059929,
     "end_time": "2023-07-23T08:46:50.259192",
     "exception": false,
     "start_time": "2023-07-23T08:46:50.199263",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Code Explanantion\n",
    "* Tokenization<br><br>\n",
    " So if you have watched the videos and referred to the links, you would know that in an RNN we input a sentence word by word. We represent every word as one hot vectors of dimensions : Numbers of words in Vocab +1. <br>\n",
    "  What keras Tokenizer does is , it takes all the unique words in the corpus,forms a dictionary with words as keys and their number of occurences as values,it then sorts the dictionary in descending order of counts. It then assigns the first value 1 , second value 2 and so on. So let's suppose word 'the' occured the most in the corpus then it will assigned index 1 and vector representing 'the' would be a one-hot vector with value 1 at position 1 and rest zereos.<br>\n",
    "  Try printing first 2 elements of xtrain_seq you will see every word is represented as a digit now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2347530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:46:50.380474Z",
     "iopub.status.busy": "2023-07-23T08:46:50.380091Z",
     "iopub.status.idle": "2023-07-23T08:46:50.386922Z",
     "shell.execute_reply": "2023-07-23T08:46:50.385339Z"
    },
    "papermill": {
     "duration": 0.071356,
     "end_time": "2023-07-23T08:46:50.389304",
     "exception": false,
     "start_time": "2023-07-23T08:46:50.317948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[664,\n",
       "  65,\n",
       "  7,\n",
       "  19,\n",
       "  2262,\n",
       "  14102,\n",
       "  5,\n",
       "  2262,\n",
       "  20439,\n",
       "  6071,\n",
       "  4,\n",
       "  71,\n",
       "  32,\n",
       "  20440,\n",
       "  6620,\n",
       "  39,\n",
       "  6,\n",
       "  664,\n",
       "  65,\n",
       "  11,\n",
       "  8,\n",
       "  20441,\n",
       "  1502,\n",
       "  38,\n",
       "  6072]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_seq[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e430e",
   "metadata": {
    "papermill": {
     "duration": 0.058574,
     "end_time": "2023-07-23T08:46:50.508302",
     "exception": false,
     "start_time": "2023-07-23T08:46:50.449728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<b>Now you might be wondering What is padding? Why its done</b><br><br>\n",
    "\n",
    "Here is the answer :\n",
    "* https://www.quora.com/Which-effect-does-sequence-padding-have-on-the-training-of-a-neural-network\n",
    "* https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/\n",
    "* https://www.coursera.org/lecture/natural-language-processing-tensorflow/padding-2Cyzs\n",
    "\n",
    "Also sometimes people might use special tokens while tokenizing like EOS(end of string) and BOS(Begining of string). Here is the reason why it's done\n",
    "* https://stackoverflow.com/questions/44579161/why-do-we-do-padding-in-nlp-tasks\n",
    "\n",
    "\n",
    "The code token.word_index simply gives the dictionary of vocab that keras created for us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53c9c68",
   "metadata": {
    "papermill": {
     "duration": 0.057562,
     "end_time": "2023-07-23T08:46:50.624753",
     "exception": false,
     "start_time": "2023-07-23T08:46:50.567191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Building the Neural Network\n",
    "\n",
    "To understand the Dimensions of input and output given to RNN in keras her is a beautiful article : https://medium.com/@shivajbd/understanding-input-and-output-shape-in-lstm-keras-c501ee95c65e\n",
    "\n",
    "The first line model.Sequential() tells keras that we will be building our network sequentially . Then we first add the Embedding layer.\n",
    "Embedding layer is also a layer of neurons which takes in as input the nth dimensional one hot vector of every word and converts it into 300 dimensional vector , it gives us word embeddings similar to word2vec. We could have used word2vec but the embeddings layer learns during training to enhance the embeddings.\n",
    "Next we add an 100 LSTM units without any dropout or regularization\n",
    "At last we add a single neuron with sigmoid function which takes output from 100 LSTM cells (Please note we have 100 LSTM cells not layers) to predict the results and then we compile the model using adam optimizer \n",
    "\n",
    "* Comments on the model<br><br>\n",
    "We can see our model achieves an accuracy of 1 which is just insane , we are clearly overfitting I know , but this was the simplest model of all ,we can tune a lot of hyperparameters like RNN units, we can do batch normalization , dropouts etc to get better result. The point is we got an AUC score of 0.82 without much efforts and we know have learnt about RNN's .Deep learning is really revolutionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d6bf7",
   "metadata": {
    "papermill": {
     "duration": 0.062917,
     "end_time": "2023-07-23T08:46:50.747723",
     "exception": false,
     "start_time": "2023-07-23T08:46:50.684806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "While building our simple RNN models we talked about using word-embeddings , So what is word-embeddings and how do we get word-embeddings?\n",
    "Here is the answer :\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/6Oq70/word-representation\n",
    "* https://machinelearningmastery.com/what-are-word-embeddings/\n",
    "<br> <br>\n",
    "The latest approach to getting word Embeddings is using pretained GLoVe or using Fasttext. Without going into too much details, I would explain how to create sentence vectors and how can we use them to create a machine learning model on top of it and since I am a fan of GloVe vectors, word2vec and fasttext. In this Notebook, I'll be using the GloVe vectors. You can download the GloVe vectors from here http://www-nlp.stanford.edu/data/glove.840B.300d.zip or you can search for GloVe in datasets on Kaggle and add the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f438948",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:46:50.871596Z",
     "iopub.status.busy": "2023-07-23T08:46:50.871007Z",
     "iopub.status.idle": "2023-07-23T08:50:21.358649Z",
     "shell.execute_reply": "2023-07-23T08:50:21.357350Z"
    },
    "papermill": {
     "duration": 210.550613,
     "end_time": "2023-07-23T08:50:21.360638",
     "exception": false,
     "start_time": "2023-07-23T08:46:50.810025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196018it [03:30, 10433.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196017 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load the GloVe vectors in a dictionary:\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('/kaggle/input/glove840b300dtxt/glove.840B.300d.txt','r',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray([float(val) for val in values[1:]])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb5daaa",
   "metadata": {
    "papermill": {
     "duration": 0.181985,
     "end_time": "2023-07-23T08:50:21.723018",
     "exception": false,
     "start_time": "2023-07-23T08:50:21.541033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LSTM's\n",
    "\n",
    "## Basic Overview\n",
    "\n",
    "Simple RNN's were certainly better than classical ML algorithms and gave state of the art results, but it failed to capture long term dependencies that is present in sentences . So in 1998-99 LSTM's were introduced to counter to these drawbacks.\n",
    "\n",
    "## In Depth Understanding\n",
    "\n",
    "Why LSTM's?\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/PKMRR/vanishing-gradients-with-rnns\n",
    "* https://www.analyticsvidhya.com/blog/2017/12/fundamentals-of-deep-learning-introduction-to-lstm/\n",
    "\n",
    "What are LSTM's?\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/KXoay/long-short-term-memory-lstm\n",
    "* https://distill.pub/2019/memorization-in-rnns/\n",
    "* https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "# Code Implementation\n",
    "\n",
    "We have already tokenized and paded our text for input to LSTM's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6439fd94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:50:22.073377Z",
     "iopub.status.busy": "2023-07-23T08:50:22.072996Z",
     "iopub.status.idle": "2023-07-23T08:50:22.221482Z",
     "shell.execute_reply": "2023-07-23T08:50:22.219711Z"
    },
    "papermill": {
     "duration": 0.327528,
     "end_time": "2023-07-23T08:50:22.223748",
     "exception": false,
     "start_time": "2023-07-23T08:50:21.896220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43496/43496 [00:00<00:00, 316995.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# create an embedding matrix for the words we have in the dataset\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2deb7122",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:50:22.575429Z",
     "iopub.status.busy": "2023-07-23T08:50:22.574333Z",
     "iopub.status.idle": "2023-07-23T08:50:22.899366Z",
     "shell.execute_reply": "2023-07-23T08:50:22.898558Z"
    },
    "papermill": {
     "duration": 0.501381,
     "end_time": "2023-07-23T08:50:22.902073",
     "exception": false,
     "start_time": "2023-07-23T08:50:22.400692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1500, 300)         13049100  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               160400    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,209,601\n",
      "Trainable params: 160,501\n",
      "Non-trainable params: 13,049,100\n",
      "_________________________________________________________________\n",
      "CPU times: user 342 ms, sys: 150 ms, total: 492 ms\n",
      "Wall time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    \n",
    "    # A simple LSTM with glove embeddings and one dense layer\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "\n",
    "    model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86050dcd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T08:50:23.267056Z",
     "iopub.status.busy": "2023-07-23T08:50:23.265938Z",
     "iopub.status.idle": "2023-07-23T09:30:47.905257Z",
     "shell.execute_reply": "2023-07-23T09:30:47.904154Z"
    },
    "papermill": {
     "duration": 2425.047032,
     "end_time": "2023-07-23T09:30:48.128586",
     "exception": false,
     "start_time": "2023-07-23T08:50:23.081554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "150/150 [==============================] - 483s 3s/step - loss: 0.2115 - accuracy: 0.9300\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 478s 3s/step - loss: 0.1405 - accuracy: 0.9509\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 469s 3s/step - loss: 0.1251 - accuracy: 0.9559\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 473s 3s/step - loss: 0.1119 - accuracy: 0.9611\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 464s 3s/step - loss: 0.1009 - accuracy: 0.9626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7e6a14f8ec20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64*strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "627d955a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T09:30:48.577708Z",
     "iopub.status.busy": "2023-07-23T09:30:48.576392Z",
     "iopub.status.idle": "2023-07-23T09:31:12.493775Z",
     "shell.execute_reply": "2023-07-23T09:31:12.492152Z"
    },
    "papermill": {
     "duration": 24.144766,
     "end_time": "2023-07-23T09:31:12.496001",
     "exception": false,
     "start_time": "2023-07-23T09:30:48.351235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 24s 309ms/step\n",
      "Auc: 0.97%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2376154",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T09:31:12.952893Z",
     "iopub.status.busy": "2023-07-23T09:31:12.952058Z",
     "iopub.status.idle": "2023-07-23T09:31:12.957494Z",
     "shell.execute_reply": "2023-07-23T09:31:12.956810Z"
    },
    "papermill": {
     "duration": 0.23297,
     "end_time": "2023-07-23T09:31:12.959855",
     "exception": false,
     "start_time": "2023-07-23T09:31:12.726885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_model.append({'Model': 'LSTM','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe25a81a",
   "metadata": {
    "papermill": {
     "duration": 0.223236,
     "end_time": "2023-07-23T09:31:13.412632",
     "exception": false,
     "start_time": "2023-07-23T09:31:13.189396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Code Explanation\n",
    "\n",
    "As a first step we calculate embedding matrix for our vocabulary from the pretrained GLoVe vectors . Then while building the embedding layer we pass Embedding Matrix as weights to the layer instead of training it over Vocabulary and thus we pass trainable = False.\n",
    "Rest of the model is same as before except we have replaced the SimpleRNN By LSTM Units\n",
    "\n",
    "* Comments on the Model\n",
    "\n",
    "We now see that the model is not overfitting and achieves an auc score of 0.96 which is quite commendable , also we close in on the gap between accuracy and auc .\n",
    "We see that in this case we used dropout and prevented overfitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf17ec8",
   "metadata": {
    "papermill": {
     "duration": 0.306019,
     "end_time": "2023-07-23T09:31:13.945676",
     "exception": false,
     "start_time": "2023-07-23T09:31:13.639657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# GRU's\n",
    "\n",
    "## Basic  Overview\n",
    "\n",
    "Introduced by Cho, et al. in 2014, GRU (Gated Recurrent Unit) aims to solve the vanishing gradient problem which comes with a standard recurrent neural network. GRU's are a variation on the LSTM because both are designed similarly and, in some cases, produce equally excellent results . GRU's were designed to be simpler and faster than LSTM's and in most cases produce equally good results and thus there is no clear winner.\n",
    "\n",
    "## In Depth Explanation\n",
    "\n",
    "* https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n",
    "* https://www.coursera.org/learn/nlp-sequence-models/lecture/agZiL/gated-recurrent-unit-gru\n",
    "* https://www.geeksforgeeks.org/gated-recurrent-unit-networks/\n",
    "\n",
    "## Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ce3e030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T09:31:14.390589Z",
     "iopub.status.busy": "2023-07-23T09:31:14.389795Z",
     "iopub.status.idle": "2023-07-23T09:31:15.192868Z",
     "shell.execute_reply": "2023-07-23T09:31:15.191639Z"
    },
    "papermill": {
     "duration": 1.031891,
     "end_time": "2023-07-23T09:31:15.197852",
     "exception": false,
     "start_time": "2023-07-23T09:31:14.165961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1500, 300)         13049100  \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 1500, 300)        0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 300)               541800    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 301       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,591,201\n",
      "Trainable params: 542,101\n",
      "Non-trainable params: 13,049,100\n",
      "_________________________________________________________________\n",
      "CPU times: user 807 ms, sys: 136 ms, total: 943 ms\n",
      "Wall time: 795 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with strategy.scope():\n",
    "    # GRU with glove embeddings and two dense layers\n",
    "     model = Sequential()\n",
    "     model.add(Embedding(len(word_index) + 1,\n",
    "                     300,\n",
    "                     weights=[embedding_matrix],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "     model.add(SpatialDropout1D(0.3))\n",
    "     model.add(GRU(300))\n",
    "     model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "     model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])   \n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2860538b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T09:31:15.690014Z",
     "iopub.status.busy": "2023-07-23T09:31:15.689369Z",
     "iopub.status.idle": "2023-07-23T10:27:18.978369Z",
     "shell.execute_reply": "2023-07-23T10:27:18.976967Z"
    },
    "papermill": {
     "duration": 3363.529135,
     "end_time": "2023-07-23T10:27:18.980773",
     "exception": false,
     "start_time": "2023-07-23T09:31:15.451638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "150/150 [==============================] - 688s 5s/step - loss: 0.2082 - accuracy: 0.9308\n",
      "Epoch 2/5\n",
      "150/150 [==============================] - 665s 4s/step - loss: 0.1204 - accuracy: 0.9559\n",
      "Epoch 3/5\n",
      "150/150 [==============================] - 701s 5s/step - loss: 0.1071 - accuracy: 0.9598\n",
      "Epoch 4/5\n",
      "150/150 [==============================] - 660s 4s/step - loss: 0.0934 - accuracy: 0.9646\n",
      "Epoch 5/5\n",
      "150/150 [==============================] - 650s 4s/step - loss: 0.0844 - accuracy: 0.9680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7e6aa84a66e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain_pad, ytrain, epochs=5, batch_size=64*strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93b24612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T10:27:19.575797Z",
     "iopub.status.busy": "2023-07-23T10:27:19.574205Z",
     "iopub.status.idle": "2023-07-23T10:29:24.980413Z",
     "shell.execute_reply": "2023-07-23T10:29:24.979699Z"
    },
    "papermill": {
     "duration": 125.858863,
     "end_time": "2023-07-23T10:29:25.127745",
     "exception": false,
     "start_time": "2023-07-23T10:27:19.268882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 125s 2s/step\n",
      "Auc: 0.98%\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict(xvalid_pad)\n",
    "print(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5809cd28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T10:29:25.745883Z",
     "iopub.status.busy": "2023-07-23T10:29:25.745157Z",
     "iopub.status.idle": "2023-07-23T10:29:25.751584Z",
     "shell.execute_reply": "2023-07-23T10:29:25.750341Z"
    },
    "papermill": {
     "duration": 0.310059,
     "end_time": "2023-07-23T10:29:25.754207",
     "exception": false,
     "start_time": "2023-07-23T10:29:25.444148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores_model.append({'Model': 'GRU','AUC_Score': roc_auc(scores,yvalid)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb4cb2e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-23T10:29:26.395827Z",
     "iopub.status.busy": "2023-07-23T10:29:26.394756Z",
     "iopub.status.idle": "2023-07-23T10:29:26.404484Z",
     "shell.execute_reply": "2023-07-23T10:29:26.402918Z"
    },
    "papermill": {
     "duration": 0.325569,
     "end_time": "2023-07-23T10:29:26.407255",
     "exception": false,
     "start_time": "2023-07-23T10:29:26.081686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Model': 'SimpleRNN', 'AUC_Score': 0.8668930775808615},\n",
       " {'Model': 'LSTM', 'AUC_Score': 0.9666138464593574},\n",
       " {'Model': 'GRU', 'AUC_Score': 0.9786564484557181}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7024.072016,
   "end_time": "2023-07-23T10:29:31.457199",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-23T08:32:27.385183",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
